{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss 0.6931471805599454\n",
      "Iteration 100: Loss 0.2909786871600512\n",
      "Iteration 200: Loss 0.2625171601024813\n",
      "Iteration 300: Loss 0.25349062662937116\n",
      "Iteration 400: Loss 0.24945451105075994\n",
      "Iteration 500: Loss 0.24730873793253783\n",
      "Iteration 600: Loss 0.2460305682763593\n",
      "Iteration 700: Loss 0.2452013273203304\n",
      "Iteration 800: Loss 0.2446254000354512\n",
      "Iteration 900: Loss 0.24420271504896565\n",
      "Iteration 1000: Loss 0.24387848249777108\n",
      "Iteration 1100: Loss 0.2436210062586389\n",
      "Iteration 1200: Loss 0.24341104595103522\n",
      "Iteration 1300: Loss 0.24323638834451036\n",
      "Iteration 1400: Loss 0.24308893974354395\n",
      "Iteration 1500: Loss 0.2429631072226714\n",
      "Iteration 1600: Loss 0.2428548680475391\n",
      "Iteration 1700: Loss 0.2427612195694402\n",
      "Iteration 1800: Loss 0.24267984551882854\n",
      "Iteration 1900: Loss 0.24260890833409518\n",
      "Iteration 2000: Loss 0.24254691643475254\n",
      "Iteration 2100: Loss 0.2424926369265298\n",
      "Iteration 2200: Loss 0.24244503638292522\n",
      "Iteration 2300: Loss 0.24240323933738875\n",
      "Iteration 2400: Loss 0.24236649820561285\n",
      "Iteration 2500: Loss 0.2423341707777058\n",
      "Iteration 2600: Loss 0.24230570287050507\n",
      "Iteration 2700: Loss 0.24228061460872685\n",
      "Iteration 2800: Loss 0.24225848934122612\n",
      "Iteration 2900: Loss 0.24223896453141544\n",
      "Iteration 3000: Loss 0.24222172416968954\n",
      "Iteration 3100: Loss 0.2422064923889062\n",
      "Iteration 3200: Loss 0.24219302805070503\n",
      "Iteration 3300: Loss 0.24218112012832269\n",
      "Iteration 3400: Loss 0.24217058375129635\n",
      "Iteration 3500: Loss 0.24216125680557712\n",
      "Iteration 3600: Loss 0.24215299700313986\n",
      "Iteration 3700: Loss 0.24214567935066275\n",
      "Iteration 3800: Loss 0.24213919395884817\n",
      "Iteration 3900: Loss 0.24213344414346163\n",
      "Iteration 4000: Loss 0.242128344776848\n",
      "Iteration 4100: Loss 0.24212382085497688\n",
      "Iteration 4200: Loss 0.24211980625029011\n",
      "Iteration 4300: Loss 0.2421162426249855\n",
      "Iteration 4400: Loss 0.24211307848303923\n",
      "Iteration 4500: Loss 0.24211026834237276\n",
      "Iteration 4600: Loss 0.24210777201119504\n",
      "Iteration 4700: Loss 0.2421055539547889\n",
      "Iteration 4800: Loss 0.24210358274091545\n",
      "Iteration 4900: Loss 0.242101830553633\n",
      "Coeficientes - Tu Implementación: [-0.43071872  3.45612435]\n",
      "Intercepto - Tu Implementación: 1.0627063938309442\n",
      "Coeficientes - Scikit-Learn: [[-0.39896392  3.30008485]]\n",
      "Intercepto - Scikit-Learn: [0.98018752]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    " \n",
    "# Datos de entrada (5 samples, 2 features)\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    " \n",
    "# Entrenar con tu implementación (ajusta según tu código)\n",
    "modelo_personalizado = LogisticRegressor()  # Reemplaza con tu clase\n",
    "modelo_personalizado.fit(X, y,learning_rate=0.0001,verbose=True,num_iterations=5000)\n",
    " \n",
    "# Comparar con Scikit-Learn\n",
    "modelo_sklearn = LogisticRegression()\n",
    "modelo_sklearn.fit(X, y)\n",
    " \n",
    "# Mostrar coeficientes aprendidos\n",
    "print(\"Coeficientes - Tu Implementación:\", modelo_personalizado.weights)\n",
    "print(\"Intercepto - Tu Implementación:\", modelo_personalizado.bias)\n",
    "print(\"Coeficientes - Scikit-Learn:\", modelo_sklearn.coef_)\n",
    "print(\"Intercepto - Scikit-Learn:\", modelo_sklearn.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the Logistic Regressor model.\n",
    "\n",
    "        Attributes:\n",
    "        - weights (np.ndarray): A placeholder for the weights of the model.\n",
    "                                These will be initialized in the training phase.\n",
    "        - bias (float): A placeholder for the bias of the model.\n",
    "                        This will also be initialized in the training phase.\n",
    "        \"\"\"\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        learning_rate=0.01,\n",
    "        num_iterations=1000,\n",
    "        penalty=None,\n",
    "        l1_ratio=0.5,\n",
    "        C=1.0,\n",
    "        verbose=False,\n",
    "        print_every=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fits the logistic regression model to the data using gradient descent.\n",
    "\n",
    "        This method initializes the model's weights and bias, then iteratively updates these parameters by\n",
    "        moving in the direction of the negative gradient of the loss function (computed using the\n",
    "        log_likelihood method).\n",
    "\n",
    "        The regularization terms are added to the gradient of the loss function as follows:\n",
    "\n",
    "        - No regularization: The standard gradient descent updates are applied without any modification.\n",
    "\n",
    "        - L1 (Lasso) regularization: Adds a term to the gradient that penalizes the absolute value of\n",
    "            the weights, encouraging sparsity. The update rule for weight w_j is adjusted as follows:\n",
    "            dw_j += (C / m) * sign(w_j) - Make sure you understand this!\n",
    "\n",
    "        - L2 (Ridge) regularization: Adds a term to the gradient that penalizes the square of the weights,\n",
    "            discouraging large weights. The update rule for weight w_j is:\n",
    "            dw_j += (C / m) * w_j       - Make sure you understand this!\n",
    "\n",
    "\n",
    "        - ElasticNet regularization: Combines L1 and L2 penalties.\n",
    "            The update rule incorporates both the sign and the magnitude of the weights:\n",
    "            dw_j += l1_ratio * gradient_of_lasso + (1 - l1_ratio) * gradient_of_ridge\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.ndarray): The input features, with shape (m, n), where m is the number of examples and n is\n",
    "                            the number of features.\n",
    "        - y (np.ndarray): The true labels of the data, with shape (m,).\n",
    "        - learning_rate (float): The step size at each iteration while moving toward a minimum of the\n",
    "                            loss function.\n",
    "        - num_iterations (int): The number of iterations for which the optimization algorithm should run.\n",
    "        - penalty (str): Type of regularization (None, 'lasso', 'ridge', 'elasticnet'). Default is None.\n",
    "        - l1_ratio (float): The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
    "                            l1_ratio=0 corresponds to L2 penalty,\n",
    "                            l1_ratio=1 to L1. Only used if penalty='elasticnet'.\n",
    "                            Default is 0.5.\n",
    "        - C (float): Inverse of regularization strength; must be a positive float.\n",
    "                            Smaller values specify stronger regularization.\n",
    "        - verbose (bool): Print loss every print_every iterations.\n",
    "        - print_every (int): Period of number of iterations to show the loss.\n",
    "\n",
    "\n",
    "\n",
    "        Updates:\n",
    "        - self.weights: The weights of the model after training.\n",
    "        - self.bias: The bias of the model after training.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Obtain m (number of examples) and n (number of features)\n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "\n",
    "        # TODO: Initialize all parameters to 0\n",
    "        self.weights = []\n",
    "        for i in range(0,n):\n",
    "            self.weights.append(0)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # TODO: Complete the gradient descent code\n",
    "        # Tip: You can use the code you had in the previous practice\n",
    "        # Execute the iterative gradient descent\n",
    "        for i in range(num_iterations):  # Fill the None here\n",
    "\n",
    "            # For these two next lines, you will need to implement the respective functions\n",
    "            # Forward propagation\n",
    "            y_hat = self.predict_proba(X)\n",
    "            # Compute loss\n",
    "            loss = self.log_likelihood(y, y_hat)\n",
    "\n",
    "            # Logging\n",
    "            if i % print_every == 0 and verbose:\n",
    "                print(f\"Iteration {i}: Loss {loss}\")\n",
    "\n",
    "            # TODO: Implement the gradient values\n",
    "            # CAREFUL! You need to calculate the gradient of the loss function (*negative log-likelihood*)\n",
    "            \n",
    "            dw = [] # Derivative w.r.t. the coefficients\n",
    "            for j in range(len(X[0])):\n",
    "                w = 0\n",
    "                for h in range(len(X)):\n",
    "                    resultado = (y[h]-y_hat[h])\n",
    "                    w += resultado*X[h][j]\n",
    "                dw.append(w)\n",
    "            b = 0\n",
    "            for h in range(len(X)):\n",
    "                b += (y[h]-y_hat[h])\n",
    "                \n",
    "            \n",
    "            db = b    # Derivative w.r.t. the intercept\n",
    "\n",
    "            # Regularization:\n",
    "            # Apply regularization if it is selected.\n",
    "            # We feed the regularization method the needed values, where \"dw\" is the derivative for the\n",
    "            # coefficients, \"m\" is the number of examples and \"C\" is the regularization hyperparameter.\n",
    "            # To do this, you will need to complete each regularization method.\n",
    "            if penalty == \"lasso\":\n",
    "                dw = self.lasso_regularization(dw, m, C)\n",
    "            elif penalty == \"ridge\":\n",
    "                dw = self.ridge_regularization(dw, m, C)\n",
    "            elif penalty == \"elasticnet\":\n",
    "                dw = self.elasticnet_regularization(dw, m, C, l1_ratio)\n",
    "            dw = np.array(dw)\n",
    "            # Update parameters\n",
    "            self.weights += learning_rate * dw\n",
    "            self.bias += learning_rate * db\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicts probability estimates for all classes for each sample X.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.ndarray): The input features, with shape (m, n), where m is the number of samples and\n",
    "            n is the number of features.\n",
    "\n",
    "        Returns:\n",
    "        - A numpy array of shape (m, 1) containing the probability of the positive class for each sample.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: z is the value of the logits. Write it here (use self.weights and self.bias)\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "        # Return the associated probabilities via the sigmoid trasnformation (symmetric choice)\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predicts class labels for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.ndarray): The input features, with shape (m, n), where m is the number of samples and n\n",
    "                            is the number of features.\n",
    "        - threshold (float): Threshold used to convert probabilities into binary class labels.\n",
    "                            Defaults to 0.5.\n",
    "\n",
    "        Returns:\n",
    "        - A numpy array of shape (m,) containing the class label (0 or 1) for each sample.\n",
    "        \"\"\"\n",
    "        # TODO: Predict the class for each input data given the threshold in the argument´\n",
    "        lista = []\n",
    "        probabilities = self.predict_proba(X)\n",
    "        for i in range(0,len(X)):\n",
    "            if probabilities[i] > threshold:\n",
    "                lista.append(0)\n",
    "            else:\n",
    "                lista.append(1)\n",
    "        classification_result = np.array(lista)\n",
    "\n",
    "        return classification_result\n",
    "\n",
    "    def lasso_regularization(self, dw, m, C):\n",
    "        \"\"\"\n",
    "        Applies L1 regularization (Lasso) to the gradient during the weight update step in gradient descent.\n",
    "        L1 regularization encourages sparsity in the model weights, potentially setting some weights to zero,\n",
    "        which can serve as a form of feature selection.\n",
    "\n",
    "        The L1 regularization term is added directly to the gradient of the loss function with respect to\n",
    "        the weights. This term is proportional to the sign of each weight, scaled by the regularization\n",
    "        strength (C) and inversely proportional to the number of samples (m).\n",
    "\n",
    "        Parameters:\n",
    "        - dw (np.ndarray): The gradient of the loss function with respect to the weights, before regularization.\n",
    "        - m (int): The number of samples in the dataset.\n",
    "        - C (float): Inverse of regularization strength; must be a positive float.\n",
    "                    Smaller values specify stronger regularization.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The adjusted gradient of the loss function with respect to the weights,\n",
    "        after applying L1 regularization.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO:\n",
    "        # ADD THE LASSO CONTRIBUTION TO THE DERIVATIVE OF THE OBJECTIVE FUNCTION\n",
    "        lasso_gradient = []\n",
    "        for i in range(0,len(self.weights)):\n",
    "            if self.weights[i] > 0:\n",
    "                lasso_gradient.append(1*C/m)\n",
    "            elif self.weights[i]< 0:\n",
    "                lasso_gradient.append(-1*C/m)\n",
    "            else:\n",
    "                lasso_gradient.append(0)\n",
    "        lasso_gradient = np.array(lasso_gradient)\n",
    "        return dw + lasso_gradient\n",
    "\n",
    "    def ridge_regularization(self, dw, m, C):\n",
    "        \"\"\"\n",
    "        Applies L2 regularization (Ridge) to the gradient during the weight update step in gradient descent.\n",
    "        L2 regularization penalizes the square of the weights, which discourages large weights and helps to\n",
    "        prevent overfitting by promoting smaller and more distributed weight values.\n",
    "\n",
    "        The L2 regularization term is added to the gradient of the loss function with respect to the weights\n",
    "        as a term proportional to each weight, scaled by the regularization strength (C) and inversely\n",
    "        proportional to the number of samples (m).\n",
    "\n",
    "        Parameters:\n",
    "        - dw (np.ndarray): The gradient of the loss function with respect to the weights, before regularization.\n",
    "        - m (int): The number of samples in the dataset.\n",
    "        - C (float): Inverse of regularization strength; must be a positive float.\n",
    "                    Smaller values specify stronger regularization.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The adjusted gradient of the loss function with respect to the weights,\n",
    "                        after applying L2 regularization.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO:\n",
    "        # ADD THE RIDGE CONTRIBUTION TO THE DERIVATIVE OF THE OBJECTIVE FUNCTION\n",
    "        ridge_gradient = []\n",
    "        for i in range(0,len(self.weights)):\n",
    "                ridge_gradient.append((self.weights[i]*C)/m)\n",
    "        ridge_gradient = np.array(ridge_gradient)\n",
    "        return dw + ridge_gradient\n",
    "\n",
    "    def elasticnet_regularization(self, dw, m, C, l1_ratio):\n",
    "        \"\"\"\n",
    "        Applies Elastic Net regularization to the gradient during the weight update step in gradient descent.\n",
    "        Elastic Net combines L1 and L2 regularization, incorporating both the sparsity-inducing properties\n",
    "        of L1 and the weight shrinkage effect of L2. This can lead to a model that is robust to various types\n",
    "        of data and prevents overfitting.\n",
    "\n",
    "        The regularization term combines the L1 and L2 terms, scaled by the regularization strength (C) and\n",
    "        the mix ratio (l1_ratio) between L1 and L2 regularization. The term is inversely proportional to the\n",
    "        number of samples (m).\n",
    "\n",
    "        Parameters:\n",
    "        - dw (np.ndarray): The gradient of the loss function with respect to the weights, before regularization.\n",
    "        - m (int): The number of samples in the dataset.\n",
    "        - C (float): Inverse of regularization strength; must be a positive float.\n",
    "                     Smaller values specify stronger regularization.\n",
    "        - l1_ratio (float): The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds\n",
    "                            to L2 penalty, l1_ratio=1 to L1. Only used if penalty='elasticnet'.\n",
    "                            Default is 0.5.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The adjusted gradient of the loss function with respect to the weights,\n",
    "                      after applying Elastic Net regularization.\n",
    "        \"\"\"\n",
    "        # TODO:\n",
    "        # ADD THE RIDGE CONTRIBUTION TO THE DERIVATIVE OF THE OBJECTIVE FUNCTION\n",
    "        # Be careful! You can reuse the previous results and combine them here, but beware how you do this!\n",
    "        ridge_gradient = (C / m) * self.weights\n",
    "\n",
    "        lasso_gradient = []\n",
    "        for i in range(0,len(self.weights)):\n",
    "            if self.weights[i] > 0:\n",
    "                lasso_gradient.append(1*C/m)\n",
    "            elif self.weights[i]< 0:\n",
    "                lasso_gradient.append(-1*C/m)\n",
    "            else:\n",
    "                lasso_gradient.append(0)\n",
    "        lasso_gradient = np.array(lasso_gradient)\n",
    "        \n",
    "        elasticnet_gradient = (1-l1_ratio)*ridge_gradient + l1_ratio*lasso_gradient\n",
    "        return dw + elasticnet_gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def log_likelihood(y, y_hat):\n",
    "        \"\"\"\n",
    "        Computes the Log-Likelihood loss for logistic regression, which is equivalent to\n",
    "        computing the cross-entropy loss between the true labels and predicted probabilities.\n",
    "        This loss function is used to measure how well the model predicts the actual class\n",
    "        labels. The formula for the loss is:\n",
    "\n",
    "        L(y, y_hat) = -(1/m) * sum(y * log(y_hat) + (1 - y) * log(1 - y_hat))\n",
    "\n",
    "        where:\n",
    "        - L(y, y_hat) is the loss function,\n",
    "        - m is the number of observations,\n",
    "        - y is the actual label of the observation,\n",
    "        - y_hat is the predicted probability that the observation is of the positive class,\n",
    "        - log is the natural logarithm.\n",
    "\n",
    "        Parameters:\n",
    "        - y (np.ndarray): The true labels of the data. Should be a 1D array of binary values (0 or 1).\n",
    "        - y_hat (np.ndarray): The predicted probabilities of the data belonging to the positive class (1).\n",
    "                            Should be a 1D array with values between 0 and 1.\n",
    "\n",
    "        Returns:\n",
    "        - The computed loss value as a scalar.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Implement the loss function (log-likelihood)\n",
    "        # Ensure numerical stability by clipping predictions to avoid log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Compute the log-likelihood loss using vectorized operations\n",
    "        loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid of z, a scalar or numpy array of any size. The sigmoid function is used as the\n",
    "        activation function in logistic regression, mapping any real-valued number into the range (0, 1),\n",
    "        which can be interpreted as a probability. It is defined as 1 / (1 + exp(-z)), where exp(-z)\n",
    "        is the exponential of the negative of z.\n",
    "\n",
    "        Parameters:\n",
    "        - z (float or np.ndarray): Input value or array for which to compute the sigmoid function.\n",
    "\n",
    "        Returns:\n",
    "        - The sigmoid of z.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Implement the sigmoid function to convert the logits into probabilities\n",
    "        valores = []\n",
    "        for valor in z:\n",
    "            valores.append(1/(1+(np.exp(-valor))))\n",
    "        valores = np.array(valores)\n",
    "        return valores\n",
    "    def fit2(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        learning_rate=0.01,\n",
    "        num_iterations=1000,\n",
    "        penalty=None,\n",
    "        l1_ratio=0.5,\n",
    "        C=1.0,\n",
    "        verbose=False,\n",
    "        print_every=100,):\n",
    "\n",
    "        m, n = X.shape  # Obtiene dimensiones de la matriz de entrada\n",
    "\n",
    "        self.weights = []\n",
    "        self.bias = 0\n",
    "        for j in range(n):\n",
    "            self.weights.append(float(0))\n",
    "        self.weights = np.array(self.weights)\n",
    "\n",
    "        for iteracion in range(1,num_iterations):\n",
    "            gradiente_b = 0  # Inicializa gradiente de bias\n",
    "            gradiente_wj = np.zeros(n)  # Inicializa gradiente de pesos como un array de ceros\n",
    "            num__de_loss = 0\n",
    "\n",
    "\n",
    "            # Calculamos las predicciones de la sigmoide para todos los ejemplos\n",
    "            y_hat = self.sigmoid2(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "            if iteracion%100 == 0:\n",
    "                num__de_loss +=1\n",
    "                LOSS = self.log_likelihood2(y,y_hat)\n",
    "                \"\"\"print(LOSS) \"\"\"\n",
    "\n",
    "            # Cálculo del gradiente de b\n",
    "            gradiente_b = np.sum(y_hat - y) / m  # Promedio de la diferencia\n",
    "            \n",
    "            # Cálculo del gradiente de w\n",
    "            gradiente_wj = np.dot(X.T, (y_hat - y)) / m  # Producto matricial para sumar eficientemente\n",
    "            if penalty == \"lasso\":\n",
    "                gradiente_wj =gradiente_wj+ self.lasso_regularization(0, m, C)\n",
    "\n",
    "            elif penalty == \"ridge\":\n",
    "                gradiente_wj =gradiente_wj+ self.ridge_regularization(0, m, C)\n",
    "            elif penalty == \"elasticnet\":\n",
    "                gradiente_wj =gradiente_wj+ self.elasticnet_regularization(0, m, C, l1_ratio)\n",
    "\n",
    "            # Actualizar parámetros\n",
    "            self.weights -= learning_rate * gradiente_wj\n",
    "            self.bias -= learning_rate * gradiente_b\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "lista = np.array([1,2])\n",
    "type(lista)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CursoMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
